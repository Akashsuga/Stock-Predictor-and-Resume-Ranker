# -*- coding: utf-8 -*-
"""tcs_lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D3mu6T9k2FlgQPL2UEfDI9ni_U1Ce5Wg
"""

import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import datetime
import tensorflow as tf

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from keras.models import Sequential
from keras.layers import LSTM, Dropout, Dense
from keras.callbacks import ReduceLROnPlateau

# Set seeds for reproducibility
np.random.seed(7)
tf.random.set_seed(7)
random.seed(7)

# 1. Load Data up to today
symbol = "TCS.NS"
today = datetime.datetime.today().strftime('%Y-%m-%d')
data = yf.download(symbol, start="2010-01-01", end=today)
data = data[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()

# 2. Feature Scaling
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

# 3. Create Dataset with Multiple Features
window_size = 120  # You can try 90, 180

def create_dataset_multi(X, y_col_index, window_size=60):
    x, y = [], []
    for i in range(window_size, len(X)):
        x.append(X[i - window_size:i])
        y.append(X[i, y_col_index])  # Close price is target (index = 3)
    return np.array(x), np.array(y)

x_all, y_all = create_dataset_multi(scaled_data, y_col_index=3, window_size=window_size)
train_size = int(len(x_all) * 0.8)

x_train = x_all[:train_size]
y_train = y_all[:train_size]
x_test = x_all[train_size:]
y_test = y_all[train_size:]

# 4. Build Improved LSTM Model
model = Sequential()
model.add(LSTM(128, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dropout(0.3))
model.add(LSTM(128))
model.add(Dropout(0.3))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

# Learning Rate Scheduler
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

# 5. Train the model
history = model.fit(
    x_train, y_train,
    epochs=50,
    batch_size=32,
    validation_data=(x_test, y_test),
    callbacks=[reduce_lr],
    verbose=1
)

# 6. Predictions and Evaluation
predicted = model.predict(x_test)
predicted_actual = scaler.inverse_transform(
    np.concatenate([np.zeros((len(predicted), 3)),  # Open, High, Low dummy
                    predicted,
                    np.zeros((len(predicted), 1))], axis=1))[:, 3]

y_test_actual = scaler.inverse_transform(
    np.concatenate([np.zeros((len(y_test), 3)),
                    y_test.reshape(-1, 1),
                    np.zeros((len(y_test), 1))], axis=1))[:, 3]

# Evaluation
rmse = np.sqrt(mean_squared_error(y_test_actual, predicted_actual))
mae = mean_absolute_error(y_test_actual, predicted_actual)
r2 = r2_score(y_test_actual, predicted_actual)

print("\nModel Evaluation:")
print(f"RMSE: {rmse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"RÂ² Score: {r2:.4f}")

# 7. Visualizations
plt.figure(figsize=(14, 6))
plt.plot(y_test_actual, color='blue', label='Actual Price')
plt.plot(predicted_actual, color='red', label='Predicted Price')
plt.title(f"{symbol} Stock Price - Actual vs Predicted")
plt.xlabel("Time")
plt.ylabel("Price (INR)")
plt.legend()
plt.grid(True)
plt.show()

# 8. Save model and scaler
model.save("lstm_model_v2.h5")
np.save("scaler_v2.npy", scaler.scale_)
np.save("min_v2.npy", scaler.min_)
print("\nModel and scaler saved as lstm_model_v2.h5, scaler_v2.npy, min_v2.npy")